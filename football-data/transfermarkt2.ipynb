{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Lists Through Transfermarkt and Saving Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from os.path import basename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to extract a picture of every player in the Premier League. We have identified Transfermarkt as our target, given that each player page should have a picture. Our secondary aim is to run this in one piece of code and not to run a new command for each player or team individually.\n",
    "\n",
    "1) Locate a list of teams in the league with links to a squad list – then save these links\n",
    "\n",
    "2) Run through each squad list link and save the link to each player’s page\n",
    "\n",
    "3) Locate the player’s image and save it to our local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': \n",
    "           'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Locate a list of team links and save them_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link of the target page: https://www.transfermarkt.co.uk/premier-league/startseite/wettbewerb/GB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process League Table\n",
    "page = 'https://www.transfermarkt.co.uk/premier-league/startseite/wettbewerb/GB1'\n",
    "tree = requests.get(page, headers = headers)\n",
    "soup = BeautifulSoup(tree.content, 'html.parser')\n",
    "\n",
    "#Create an empty list to assign these values to\n",
    "teamLinks = []\n",
    "\n",
    "#Extract all links with the correct CSS selector\n",
    "links = soup.select(\"a.vereinprofil_tooltip\")\n",
    "\n",
    "#We need the location that the link is pointing to, so for each link, take the link location. \n",
    "#Additionally, we only need the links in locations 1,3,5,etc. of our list, so loop through those only\n",
    "for i in range(1,41,2):\n",
    "    teamLinks.append(links[i].get(\"href\"))\n",
    "    \n",
    "#For each location that we have taken, add the website before it - this allows us to call it later\n",
    "for i in range(len(teamLinks)):\n",
    "    teamLinks[i] = \"https://www.transfermarkt.co.uk\"+teamLinks[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run through each squad and save the player links_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamLinks[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now iterate through each of these team links and do the same thing, only this time we are taking player links and not squad links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list for our player links to go into\n",
    "playerLinks = []\n",
    "\n",
    "#Run the scraper through each of our 20 team links\n",
    "for i in range(len(teamLinks)):\n",
    "\n",
    "    #Download and process the team page\n",
    "    page = teamLinks[i]\n",
    "    tree = requests.get(page, headers = headers)\n",
    "    soup = BeautifulSoup(tree.content, 'html.parser')\n",
    "\n",
    "    #Extract all links\n",
    "    links = soup.select(\"a.spielprofil_tooltip\")\n",
    "    \n",
    "    #For each link, extract the location that it is pointing to\n",
    "    for j in range(len(links)):\n",
    "        playerLinks.append(links[j].get(\"href\"))\n",
    "\n",
    "    #Add the location to the end of the transfermarkt domain to make it ready to scrape\n",
    "    for j in range(len(playerLinks)):\n",
    "        playerLinks[j] = \"https://www.transfermarkt.co.uk\"+playerLinks[j]\n",
    "\n",
    "    #The page list the players more than once - let's use list(set(XXX)) to remove the duplicates\n",
    "    playerLinks = list(set(playerLinks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Locate and save each player’s image_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(playerLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we are locating elements in the page. When we try to identify the correct image on the page, it seems that the best way to do this is through the ‘title’ attribute – which is the player’s name. It is ridiculous for us to manually enter the name for each one, so we need to find this elsewhere on the page. Fortunately, it is easy to find this as it is the only ‘h1’ elemnt.\n",
    "\n",
    "Subsequently, we assign this name to the name variable, then use it to call the correct image.\n",
    "\n",
    "When we call the image, we actually need to call the location where the image is saved on the website’s server. We do this by calling for the image’s source. The source contains some extra information that we don’t need, so we use .split() to isolate the information that we do need and save that to our ‘src’ variable.\n",
    "\n",
    "The final thing to do is to save the image from this source location. We do this by opening a new file named after the player, then save the content from source to the new file. Incredibly, Python does this in just two lines. All images will be saved into the folder that your Python notebook or file is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(playerLinks)):\n",
    "\n",
    "    #Take site and structure html\n",
    "    page = playerLinks[i]\n",
    "    tree = requests.get(page, headers=headers)\n",
    "    soup = BeautifulSoup(tree.content, 'html.parser')\n",
    "\n",
    "\n",
    "    #Find image and save it with the player's name\n",
    "    #Find the player's name\n",
    "    name = soup.find_all(\"h1\")\n",
    "    \n",
    "    #Use the name to call the image\n",
    "    image = soup.find_all(\"img\",{\"title\":name[0].text})\n",
    "    \n",
    "    #Extract the location of the image. We also need to strip the text after '?lm', so let's do that through '.split()'.\n",
    "    src = image[0].get('src').split(\"?lm\")[0]\n",
    "\n",
    "    #Save the image under the player's name\n",
    "    with open(name[0].text+\".jpg\",\"wb\") as f:\n",
    "        f.write(requests.get(src).content)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7894a52d68d58fbd92560d3706c1758f91e373e9093849d544b4761ad70d7c45"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('webScraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
